{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:11.085739Z",
     "start_time": "2024-12-05T22:39:11.080900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ],
   "id": "1c0d17fbe918d08",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deel 1: preprocessing\n",
    "De data die we gaan gebruiken is (weer) de wiki-pagina over kanker. De tekst hiervan kun je hier downloaden. Gebruik van alle zinnen die je hierin vindt alleen de zinnen die meer dan tien woorden bevatten. Voeg deze zinnen samen in een lijst data. Als het goed is, is len(data)=59."
   ],
   "id": "30bf1979addef15d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:11.106615Z",
     "start_time": "2024-12-05T22:39:11.100593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with(open(\"wiki.txt\", \"r\")) as file:\n",
    "    wiki_text = [line.strip() for line in file if len(line.strip().split()) >= 10]\n",
    "\n",
    "print(f\"wiki_text len: {len(wiki_text)}\")\n",
    "    "
   ],
   "id": "a08f0b528483923d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_text len: 62\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Maak vervolgens een functie preprocess_sentence die een string meekrijgt en die die string opgeschoond teruggeeft. Het opschonen van de string bestaat uit de volgende twee stappen:\n",
    "\n",
    "+ verwijderen van (in ieder geval) de volgende karakters: /, ., ., ', ,, \", :, ;, (, ) (misschien dat er nog andere karakters zijn die je uit de zinnen wilt halen).\n",
    "+ verwijderen van de stopwoorden uit de zin; dit zijn woorden die wel nodig zijn voor de grammatica, maar niet per se om de context te bepalen. Je kunt de stopwoorden hier downloaden."
   ],
   "id": "e45baac8253fa7fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:11.250516Z",
     "start_time": "2024-12-05T22:39:11.238782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with(open(\"stopwoorden.txt\", \"r\")) as file:\n",
    "    stopwoorden = [line.strip() for line in file if line]\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    to_exclude = \"/..\\',\\\":;()[]0123456789\"\n",
    "    sentence = \" \".join([word for word in sentence.split() if word.lower() not in stopwoorden])\n",
    "    sentence = \"\".join([letter if letter not in to_exclude else \" \" for letter in sentence])\n",
    "\n",
    "    return sentence\n"
   ],
   "id": "452f01253e959f09",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Roep nu de functie preprocess_data aan met alle zinnen uit data. Sla het resultaat op in een nieuwe variabele (bijvoorbeeld sentences of corpus). Deze variabele is je corpus.",
   "id": "356496bc8a97260c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:11.301693Z",
     "start_time": "2024-12-05T22:39:11.280820Z"
    }
   },
   "cell_type": "code",
   "source": "corpus  = [preprocess_sentence(sentence) for sentence in wiki_text]",
   "id": "e2885509477c53db",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Een tweede stap die we moeten zetten is het samenstellen van het vocabulaire: de woorden waaruit onze zinnen zijn samengesteld (net zoals het Nederlandse woordenboek alle woorden bevat waaruit alle Nederlandse zinnen zijn samengesteld). Sla dit op in een tweede variabele (standaard heet dat ding vocab). Als het goed is, zijn er 717 woorden in je vocabulaire.",
   "id": "e57c5305e0689633"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:11.355886Z",
     "start_time": "2024-12-05T22:39:11.349411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = list(set(\" \".join(corpus).split()))\n",
    "print(f\"vocab len: {len(vocab)}\")"
   ],
   "id": "4f734bcf154c627d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 776\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stap 2: CBOW paren aanmaken\n",
    "Maak nu een functie create_pairs die alle zinnen uit het corpus meekrijgt (corpus), en een parameter (w_size) die aangeeft hoe groot het window moet zijn waarmee het algoritme door het corpus loopt (2, in het voorbeeld hierboven). Deze functie loopt per zin met stappen van w_size over de woorden w van de zin heen. Elke iteratie worden de w_size aan de linkerkant én aan de rechterkant van het woord w als context (als input als het ware) gezien en het woord w zelf als output (zie eventueel de beschrijving hierboven). Sla uiteindelijk alle contexten op in een matrix X en alle woorden w in een vector y. Retourneer X en y."
   ],
   "id": "89777654628bc0af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:41:44.062687Z",
     "start_time": "2024-12-05T22:41:44.054043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_pairs(corpus, w_size):\n",
    "    X = []  # Context_word (input)\n",
    "    y = []  # Target_word (output)\n",
    "\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()  # Verdeel de zin in woorden\n",
    "        for index, target_word in enumerate(words):\n",
    "            # Bepaal de context range (links en rechts van target_word)\n",
    "            start = max(index - w_size, 0)\n",
    "            end = min(index + w_size + 1, len(words))\n",
    "\n",
    "            # Maak een lijst met alle woorden in het window, behalve target_word zelf\n",
    "            context_words = [words[i] for i in range(start, end) if i != index]\n",
    "\n",
    "            # Voeg de context en target toe aan de lijsten\n",
    "            X.append(\" \".join(context_words))  # Combineer contextwoorden tot een string\n",
    "            y.append(target_word)              # Doelwoord\n",
    "\n",
    "    return X, y\n"
   ],
   "id": "34d31453ce31d90c",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Roep nu de functie create_pairs aan met je corpus en een w_size van 2, 3 of 4. Nu hebben we een dikke matrix X en een dikke vector y, met allemaal woorden erin. Omdat netwerken niet (of in ieder geval niet goed) met strings om kunnen gaan, moeten we deze omzetten in ijle matrices van getallen. Nu kunnen we dat óók wel zelf doen, maar in dit geval is het voldoende om gebruik te maken van de klasse CountVectorizer van sklearn. Maak vervolgens gebruik van test_train_split om deze matrices om te zetten in trainingsdata en testdata. Zie de voorbeeldcode hieronder:\n",
    "\n",
    "```\n",
    "vectorizer = CountVectorizer(max_features=len(voc), tokenizer=lambda x: x.split())\n",
    "X_sentences = vectorizer.fit_transform(sentences).toarray()\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "```"
   ],
   "id": "b8ed7cbfb1d7751b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:44:10.286275Z",
     "start_time": "2024-12-05T22:44:10.263054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = create_pairs(corpus, 2)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=len(vocab), tokenizer=lambda x: x.split())\n",
    "X_sentences = vectorizer.fit_transform(X).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sentences, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "9f61afb503b07e8f",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stap 3: het maken en trainen van het model\n",
    "Scikit-learn heeft geen netwerk dat we direct kunnen inzetten voor het maken van een CBOW-model. We kunnen natuurlijk een heel netwerk samenstellen, maar de MLPClassifier vormt voor deze exercitie een voldoende benadering. Maak een object aan van deze klasse met zo'n honderd verborgen nodes in de verborgen laag. Train het model op de trainingsdata."
   ],
   "id": "c2e0798b4b000732"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:24.495472Z",
     "start_time": "2024-12-05T22:39:11.511140Z"
    }
   },
   "cell_type": "code",
   "source": "classifier = MLPClassifier(hidden_layer_sizes=100, max_iter=1000).fit(X_train, y_train)",
   "id": "76637f96f9169e63",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stap 4: model-evaluatie\n",
    "Test je getrainde model met de testdata die je in de vorige stap hebt gemaakt. Je zult zien dat de accuratesse van het netwerk niet veel hoger komt dan twintig procent. Waardeloos, natuurlijk, maar wel verklaarbaar gezien de relatief beperkte omvang van onze dataset.\n",
    "\n"
   ],
   "id": "d497f3bfb9244730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:24.588937Z",
     "start_time": "2024-12-05T22:39:24.556379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy: {accuracy : .4f}\")\n"
   ],
   "id": "f463c527a901bb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.0145\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stap 5: de embedding layer\n",
    "De getrainde MLPClassifier slaat de getrainde waarden op in coef_: de eerste waarde in deze lijst bevat de gewichten tussen de input-laag en de verborgen laag, terwijl de tweede waarde de gewichten bevat tussen de verborgen laag en de output-laag. Maak een dictionary met als keys de waarden uit je vocabulaire en als values de corresponderende waarden in de gewichtenmatrix. Als je dit hebt gedaan, kun je bijvoorbeeld `word_vector['kanker']` opvragen. Maak tenslotte gebruik van np.linalg.norm om de vectoren van alle woorden te normaliseren.\n",
    "\n"
   ],
   "id": "5849775c6fe54a92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:39:24.652650Z",
     "start_time": "2024-12-05T22:39:24.615653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_vector = {}\n",
    "\n",
    "weights_to_hidden = classifier.coefs_[0]\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word_vector[word] = weights_to_hidden[i]\n",
    "\n"
   ],
   "id": "595a6ca015583f9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 746 is out of bounds for axis 0 with size 746",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(vocab))\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(vocab):\n\u001B[0;32m----> 8\u001B[0m     word_vector[word] \u001B[38;5;241m=\u001B[39m \u001B[43mweights_to_hidden\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 746 is out of bounds for axis 0 with size 746"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed66f82959881a92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
