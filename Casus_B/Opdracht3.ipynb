{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:30:23.815936Z",
     "start_time": "2024-12-04T12:30:23.744979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Cython.Shadow import typeof\n",
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "1c0d17fbe918d08",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deel 1: preprocessing\n",
    "De data die we gaan gebruiken is (weer) de wiki-pagina over kanker. De tekst hiervan kun je hier downloaden. Gebruik van alle zinnen die je hierin vindt alleen de zinnen die meer dan tien woorden bevatten. Voeg deze zinnen samen in een lijst data. Als het goed is, is len(data)=59."
   ],
   "id": "30bf1979addef15d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:30:23.831682Z",
     "start_time": "2024-12-04T12:30:23.825660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with(open(\"wiki.txt\", \"r\")) as file:\n",
    "    wiki_text = [line.strip() for line in file if len(line.strip().split()) >= 10]\n",
    "\n",
    "print(f\"wiki_text len: {len(wiki_text)}\")\n",
    "    "
   ],
   "id": "a08f0b528483923d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_text len: 62\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Maak vervolgens een functie preprocess_sentence die een string meekrijgt en die die string opgeschoond teruggeeft. Het opschonen van de string bestaat uit de volgende twee stappen:\n",
    "\n",
    "+ verwijderen van (in ieder geval) de volgende karakters: /, ., ., ', ,, \", :, ;, (, ) (misschien dat er nog andere karakters zijn die je uit de zinnen wilt halen).\n",
    "+ verwijderen van de stopwoorden uit de zin; dit zijn woorden die wel nodig zijn voor de grammatica, maar niet per se om de context te bepalen. Je kunt de stopwoorden hier downloaden."
   ],
   "id": "e45baac8253fa7fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:30:23.889506Z",
     "start_time": "2024-12-04T12:30:23.881665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with(open(\"stopwoorden.txt\", \"r\")) as file:\n",
    "    stopwoorden = [line.strip() for line in file if line]\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    to_exclude = \"/..\\',\\\":;()[]0123456789\"\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in stopwoorden])\n",
    "    sentence = \"\".join([letter if letter not in to_exclude else \" \" for letter in sentence])\n",
    "\n",
    "    return sentence\n"
   ],
   "id": "452f01253e959f09",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Roep nu de functie preprocess_data aan met alle zinnen uit data. Sla het resultaat op in een nieuwe variabele (bijvoorbeeld sentences of corpus). Deze variabele is je corpus.",
   "id": "356496bc8a97260c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:30:23.952240Z",
     "start_time": "2024-12-04T12:30:23.937439Z"
    }
   },
   "cell_type": "code",
   "source": "corpus  = [preprocess_sentence(sentence) for sentence in wiki_text]",
   "id": "e2885509477c53db",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Een tweede stap die we moeten zetten is het samenstellen van het vocabulaire: de woorden waaruit onze zinnen zijn samengesteld (net zoals het Nederlandse woordenboek alle woorden bevat waaruit alle Nederlandse zinnen zijn samengesteld). Sla dit op in een tweede variabele (standaard heet dat ding vocab). Als het goed is, zijn er 717 woorden in je vocabulaire.",
   "id": "e57c5305e0689633"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:30:24.022048Z",
     "start_time": "2024-12-04T12:30:24.011263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = list(set(\" \".join(corpus).split()))\n",
    "print(f\"vocab len: {len(vocab)}\")"
   ],
   "id": "4f734bcf154c627d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 819\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stap 2: CBOW paren aanmaken\n",
    "Maak nu een functie create_pairs die alle zinnen uit het corpus meekrijgt (corpus), en een parameter (w_size) die aangeeft hoe groot het window moet zijn waarmee het algoritme door het corpus loopt (2, in het voorbeeld hierboven). Deze functie loopt per zin met stappen van w_size over de woorden w van de zin heen. Elke iteratie worden de w_size aan de linkerkant én aan de rechterkant van het woord w als context (als input als het ware) gezien en het woord w zelf als output (zie eventueel de beschrijving hierboven). Sla uiteindelijk alle contexten op in een matrix X en alle woorden w in een vector y. Retourneer X en y."
   ],
   "id": "89777654628bc0af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:30:24.107677Z",
     "start_time": "2024-12-04T12:30:24.096030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_pairs(corpus, w_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in corpus: # per zin\n",
    "        sentence = sentence.split()\n",
    "        for i in range(w_size, len(sentence) - w_size): # per woord\n",
    "            contex = []\n",
    "            for j in range(w_size * 2 +1): # context index\n",
    "                if j -w_size != 0: # niet y woord\n",
    "                    contex.append(sentence[i + j -w_size])\n",
    "            \n",
    "            X.append(contex)\n",
    "            y.append(sentence[i])\n",
    "    return X, y\n"
   ],
   "id": "34d31453ce31d90c",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Roep nu de functie create_pairs aan met je corpus en een w_size van 2, 3 of 4. Nu hebben we een dikke matrix X en een dikke vector y, met allemaal woorden erin. Omdat netwerken niet (of in ieder geval niet goed) met strings om kunnen gaan, moeten we deze omzetten in ijle matrices van getallen. Nu kunnen we dat óók wel zelf doen, maar in dit geval is het voldoende om gebruik te maken van de klasse CountVectorizer van sklearn. Maak vervolgens gebruik van test_train_split om deze matrices om te zetten in trainingsdata en testdata. Zie de voorbeeldcode hieronder:\n",
    "\n",
    "```\n",
    "vectorizer = CountVectorizer(max_features=len(voc), tokenizer=lambda x: x.split())\n",
    "X_sentences = vectorizer.fit_transform(sentences).toarray()\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "```"
   ],
   "id": "b8ed7cbfb1d7751b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T12:34:53.442977Z",
     "start_time": "2024-12-04T12:34:53.383308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = create_pairs(corpus, 3)\n",
    "\n",
    "context_strings = [\" \".join(context) for context in X]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_sentences = vectorizer.fit_transform(context_strings).toarray()\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sentences, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train) \n",
    "        "
   ],
   "id": "9f61afb503b07e8f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'typeof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[85], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m vocab \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mget_feature_names_out()\n\u001B[1;32m      9\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X_sentences, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mprint\u001B[39m(X_train\u001B[38;5;241m.\u001B[39mshape, \u001B[43mtypeof\u001B[49m(X_test), y_train) \n",
      "\u001B[0;31mNameError\u001B[0m: name 'typeof' is not defined"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "efbe38d134d05a5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
